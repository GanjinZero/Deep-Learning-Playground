{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers包使用教程\n",
    "Author:Yuan Zheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- 预训练模型介绍\n",
    "- 安装Transformers\n",
    "- 加载预训练模型\n",
    "- fine tune预训练模型\n",
    "- 训练自己的下游模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练模型介绍\n",
    "## Bert\n",
    "英文模型、中文模型\n",
    "最近更新了wwm和小参数模型\n",
    "https://github.com/google-research/bert\n",
    "\n",
    "### Whole-word-masking\n",
    "Original Sentence: 我今天吃饭了。\n",
    "Mask: 我今\\[MASK\\]天吃饭了。\n",
    "Whole-word-masking: 我\\[MASK\\]\\[MASK\\]吃饭了。\n",
    "\n",
    "bert-wwm\n",
    "roberta-wwm\n",
    "https://github.com/ymcui/Chinese-BERT-wwm\n",
    "\n",
    "### BioBert\n",
    "用PubMed + ... 训练的英文bert\n",
    "https://huggingface.co/monologg/biobert_v1.1_pubmed\n",
    "\n",
    "## Roberta\n",
    "改变了一些预训练细节，包括mask和batch等。\n",
    "https://github.com/ymcui/Chinese-BERT-wwm\n",
    "\n",
    "## Albert\n",
    "参数共享，更小的模型文件，更好的表现。\n",
    "https://github.com/google-research/albert\n",
    "\n",
    "# 下载地址\n",
    "/media/sdc/GanjinZero/pretraining_models/\n",
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装transformers\n",
    "- 需要Tensorflow 2.0+或者Pytorch 1.0+\n",
    "- 主要还是用的是Pytorch\n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0327 12:16:26.964957 140463573395200 file_utils.py:41] PyTorch version 1.2.0 available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载预训练模型\n",
    "预训练模型在transformers包中有三个组成部分：模型结构(config)、预训练文件(checkpoint)、词典(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0327 12:39:07.922970 140463573395200 configuration_utils.py:254] loading configuration file /media/sdc/GanjinZero/pretraining_models/bert_wwm/config.json\n",
      "I0327 12:39:07.924646 140463573395200 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0327 12:39:07.925683 140463573395200 tokenization_utils.py:417] Model name '/media/sdc/GanjinZero/pretraining_models/bert_wwm' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/media/sdc/GanjinZero/pretraining_models/bert_wwm' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0327 12:39:07.927194 140463573395200 tokenization_utils.py:446] Didn't find file /media/sdc/GanjinZero/pretraining_models/bert_wwm/added_tokens.json. We won't load it.\n",
      "I0327 12:39:07.928002 140463573395200 tokenization_utils.py:446] Didn't find file /media/sdc/GanjinZero/pretraining_models/bert_wwm/special_tokens_map.json. We won't load it.\n",
      "I0327 12:39:07.928731 140463573395200 tokenization_utils.py:446] Didn't find file /media/sdc/GanjinZero/pretraining_models/bert_wwm/tokenizer_config.json. We won't load it.\n",
      "I0327 12:39:07.929330 140463573395200 tokenization_utils.py:499] loading file /media/sdc/GanjinZero/pretraining_models/bert_wwm/vocab.txt\n",
      "I0327 12:39:07.929948 140463573395200 tokenization_utils.py:499] loading file None\n",
      "I0327 12:39:07.930589 140463573395200 tokenization_utils.py:499] loading file None\n",
      "I0327 12:39:07.931193 140463573395200 tokenization_utils.py:499] loading file None\n",
      "I0327 12:39:07.951953 140463573395200 configuration_utils.py:254] loading configuration file /media/sdc/GanjinZero/pretraining_models/bert_wwm/config.json\n",
      "I0327 12:39:07.952908 140463573395200 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0327 12:39:07.953807 140463573395200 modeling_utils.py:459] loading weights file /media/sdc/GanjinZero/pretraining_models/bert_wwm/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# 加载一个官方预训练好的模型，需要在线下载\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer # 自动识别模型\n",
    "## Bert\n",
    "\"\"\"\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenzier = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\"\"\"\n",
    "\n",
    "## BioBert\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "model = AutoModel.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "\"\"\"\n",
    "\n",
    "## Roberta\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\"\"\"\n",
    "\n",
    "## Albert\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"voidful/albert_chinese_base\")\n",
    "model = AutoModel.from_pretrained(\"voidful/albert_chinese_base\")\n",
    "\"\"\"\n",
    "\n",
    "# 加载一个本地的模型（可能是你fine—tune过的）\n",
    "# 加载bert_wwm_\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/media/sdc/GanjinZero/pretraining_models/bert_wwm\")\n",
    "model = AutoModel.from_pretrained(\"/media/sdc/GanjinZero/pretraining_models/bert_wwm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([1, 9, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sentence = \"孙一峰二五仔。\"\n",
    "print(len(sentence))\n",
    "input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    output_seq = output[0]\n",
    "    output_cls = output[1]\n",
    "print(output_seq.shape) #用于字符分类\n",
    "print(output_cls.shape) #用于句子分类\n",
    "(output_seq[0][0] == output_cls[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类模型\n",
    "run_glue.py\n",
    "https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n",
    "\n",
    "```shell\n",
    "export GLUE_DIR=/path/to/glue\n",
    "export TASK_NAME=MRPC\n",
    "\n",
    "python ./examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=8   \\\n",
    "    --per_gpu_train_batch_size=8   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --output_dir /tmp/$TASK_NAME/\n",
    "```\n",
    "参数解释：\n",
    "- model_type 模型类型\n",
    "- model_name_or_path 模型地址\n",
    "- task_name 任务名称\n",
    "- do_train 是否训练\n",
    "- do_eval 是否验证\n",
    "- do_lower_case 是否小写（和模型有关系）\n",
    "- data_dir 数据集文件夹\n",
    "- max_seq_length 文本最长长度\n",
    "- learngin_rate 学习率：建议1e-5~5e-5\n",
    "- num_train_epochs epoch：建议2~5\n",
    "- batch_size 看如下表格\n",
    "\n",
    "12GB显存\n",
    "System       | Seq Length | Max Batch Size\n",
    "------------ | ---------- | --------------\n",
    "`BERT-Base`  | 64         | 64\n",
    "...          | 128        | 32\n",
    "...          | 256        | 16\n",
    "...          | 320        | 14\n",
    "...          | 384        | 12\n",
    "...          | 512        | 6\n",
    "`BERT-Large` | 64         | 12\n",
    "...          | 128        | 6\n",
    "...          | 256        | 2\n",
    "...          | 320        | 1\n",
    "...          | 384        | 0\n",
    "...          | 512        | 0\n",
    "据说batch size过小会影响效果\n",
    "\n",
    "对于自己的任务，需要改写预处理数据的文件！或者将自己的数据弄得和他格式一致。\n",
    "比如说把你的任务是双句子二分类任务（比如句子相似任务），看MRPC数据集在transformers里的读入代码：\n",
    "\n",
    "```python\n",
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()),\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "```\n",
    "\n",
    "你需要在data_dir中存有train.csv, dev.csv和test.csv。其中文件每行为：xxx就是随意，+代表tab\n",
    "label + xxx + xxx + text_a + text_b\n",
    "\n",
    "如果你是单句子二分类任务，可以表示成\n",
    "label + xxx + xxx + text_a + \"\"\n",
    "\n",
    "主要目的就是为了凑\n",
    "```python\n",
    "text_a = line[3]\n",
    "text_b = line[4]\n",
    "label = line[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于需要完全改写预处理的文件格式，注释掉run_glue.py的这四行\n",
    "```python\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "```\n",
    "按照你自己的文件格式重写这4个函数，可以参照transformers里的源代码改写。\n",
    "https://github.com/huggingface/transformers/tree/master/src/transformers/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine tune后的模型和预测结构都保存在output文件夹中。如果想要再使用模型，读取该文件夹中的保存模型即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练自己的下游模型\n",
    "在这里举一个例子，如何利用Transformers搭建自己的模型。\n",
    "模型目标：判断两个句子是否相似。\n",
    "采用的模型结构：Siamese-Bert\n",
    "完整的模型代码：/media/sdc/GanjinZero/bert-siamese\n",
    "模型描述：用Bert作为句子的encoder，对于两个句子text_a, text_b；得到Bert(text_a), Bert(text_b)。用(Bert(text_a), Bert(text_b), |Bert(text_a) - Bert(text_b)|)连接一个全连接层预测两个句子的相似与否。Loss使用CrossEntropy(MSE当然也可以)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertPreTrainedModel, BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertSiamese(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob) \n",
    "        self.classifier = nn.Linear(3 * config.hidden_size, 2) # 1 for regression\n",
    "        torch.nn.init.normal_(self.classifier.weight)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_0=None,\n",
    "        attention_mask_0=None,\n",
    "        token_type_ids_0=None,\n",
    "        position_ids_0=None,\n",
    "        head_mask_0=None,\n",
    "        inputs_embeds_0=None,\n",
    "        input_ids_1=None,\n",
    "        attention_mask_1=None,\n",
    "        token_type_ids_1=None,\n",
    "        position_ids_1=None,\n",
    "        head_mask_1=None,\n",
    "        inputs_embeds_1=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs_0 = self.bert(\n",
    "            input_ids_0,\n",
    "            attention_mask=attention_mask_0,\n",
    "            token_type_ids=token_type_ids_0,\n",
    "            position_ids=position_ids_0,\n",
    "            head_mask=head_mask_0,\n",
    "            inputs_embeds=inputs_embeds_0)\n",
    "        \n",
    "        outputs_1 = self.bert(\n",
    "            input_ids_1,\n",
    "            attention_mask=attention_mask_1,\n",
    "            token_type_ids=token_type_ids_1,\n",
    "            position_ids=position_ids_1,\n",
    "            head_mask=head_mask_1,\n",
    "            inputs_embeds=inputs_embeds_1)\n",
    "\n",
    "        pooled_output_0 = outputs_0[1]\n",
    "        pooled_output_1 = outputs_1[1]\n",
    "        minus = torch.abs(pooled_output_0 - pooled_output_1)\n",
    "        h = torch.cat((pooled_output_0, pooled_output_1, minus), 1)\n",
    "        h = self.dropout(h)\n",
    "        logits = self.classifier(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.CrossEntropyLoss()(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        else:\n",
    "            outputs = (0, logits)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0327 13:29:11.108398 140463573395200 configuration_utils.py:254] loading configuration file /media/sdc/GanjinZero/pretraining_models/bert_wwm/config.json\n",
      "I0327 13:29:11.110187 140463573395200 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0327 13:29:11.112348 140463573395200 modeling_utils.py:459] loading weights file /media/sdc/GanjinZero/pretraining_models/bert_wwm/pytorch_model.bin\n",
      "I0327 13:29:13.077636 140463573395200 modeling_utils.py:546] Weights of BertSiamese not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0327 13:29:13.078680 140463573395200 modeling_utils.py:552] Weights from pretrained model not used in BertSiamese: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.6920, grad_fn=<NllLossBackward>), tensor([[-1.0064, -0.1961],\n",
      "        [-0.9117, -0.0814],\n",
      "        [-0.7019, -0.3580]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "model = BertSiamese.from_pretrained('/media/sdc/GanjinZero/pretraining_models/bert_wwm')\n",
    "\n",
    "input_ids_0 = torch.tensor(tokenizer.encode(\"黄旭东扎色a\", add_special_tokens=True)).unsqueeze(0)\n",
    "input_ids_1 = torch.tensor(tokenizer.encode(\"孙一峰二五仔\", add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "output_label = model(input_ids_0=torch.cat((input_ids_0, input_ids_1, input_ids_0), 0), \n",
    "                     input_ids_1=torch.cat((input_ids_1, input_ids_1, input_ids_0), 0),\n",
    "                     labels=torch.tensor([0, 1, 1]))\n",
    "print(output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids_0, input_ids_1, attention_mask_0=None, attention_mask_1=None, token_type_ids_0=None, token_type_ids_1=None, label=None):\n",
    "        self.input_ids_0 = input_ids_0\n",
    "        self.attention_mask_0 = attention_mask_0\n",
    "        self.token_type_ids_0 = token_type_ids_0\n",
    "        self.input_ids_1 = input_ids_1\n",
    "        self.attention_mask_1 = attention_mask_1\n",
    "        self.token_type_ids_1 = token_type_ids_1\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.data.processors import DataProcessor\n",
    "\n",
    "\n",
    "class SiameseProcessor(DataProcessor):\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.csv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"1\", \"0\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s-%s-%s\" % (set_type, line[1], line[2], line[3])\n",
    "            text_a = line[1]\n",
    "            text_b = line[2]\n",
    "            label = line[-1]\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine tune细节\n",
    "一般会用学习率在训练过程中是在改变的。先线性上升(warmup)，再逐步归零。所以训练过程中注意有这个步骤。详细训练代码见/media/sdc/GanjinZero/bert-siamese/train.py\n",
    "核心训练代码见下面，不和上面接着！这是DuReader一个QA任务的train代码，写的比较短就拿过来了。这个任务的完整代码见https://github.com/basketballandlearn/Dureader-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import args\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import evaluate\n",
    "from dataset.dataloader import Dureader\n",
    "from transformers import BertForQuestionAnswering, BertConfig, AdamW\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 加载预训练bert\n",
    "    model = BertForQuestionAnswering.from_pretrained('/media/sdc/GanjinZero/pretraining_models/bert_wwm')\n",
    "    device = args.device\n",
    "    model.to(device)\n",
    "\n",
    "    # 准备 optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "            ]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=0.1, t_total=args.num_train_optimization_steps)\n",
    "\n",
    "    # 准备数据\n",
    "    data = Dureader()\n",
    "    train_dataloader, dev_dataloader = data.train_iter, data.dev_iter\n",
    "\n",
    "    best_loss = 100000.0\n",
    "    model.train()\n",
    "    for i in range(args.num_train_epochs):\n",
    "        for step , batch in enumerate(tqdm(train_dataloader, desc=\"Epoch\")):\n",
    "            input_ids, input_mask, segment_ids, start_positions, end_positions = \\\n",
    "                                        batch.input_ids, batch.input_mask, batch.segment_ids, batch.start_position, batch.end_position\n",
    "            input_ids, input_mask, segment_ids, start_positions, end_positions = \\\n",
    "                                        input_ids.to(device), input_mask.to(device), segment_ids.to(device), start_positions.to(device), end_positions.to(device)\n",
    "\n",
    "            # 计算loss\n",
    "            loss, _, _ = model(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新梯度\n",
    "            if (step+1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # 验证\n",
    "            if step % args.log_step == 4:\n",
    "                eval_loss = evaluate.evaluate(model, dev_dataloader)\n",
    "                if eval_loss < best_loss:\n",
    "                    best_loss = eval_loss\n",
    "                    torch.save(model.state_dict(), './model_dir/' + \"best_model\")\n",
    "                    model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
